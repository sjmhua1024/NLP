{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分词技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 规则分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向最大匹配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆向最大匹配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双向最大匹配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>步骤</b>：\n",
    "\n",
    "1. 建立统计语言模型。\n",
    "\n",
    "2. 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。这里就用到了统计学习方法，如隐马尔科夫模型（HMM）、条件随机场（CRF）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>n元模型（n-gram model）</b>：\n",
    "- n=1时，一元模型（unigram model）\n",
    "- n=2时，二元模型（bigram model）\n",
    "- n=3时，三元模型（trigram model）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本思路：    \n",
    "每个字在构造一个特定的词语时都占据着一个确定的构词位置（词位），规定每个字最多只有四个构词位置：B（词首）、M（词中）、E（词尾）、S（单独成词）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义HMM类\n",
    "class HMM(object):\n",
    "    #构造函数\n",
    "    def __init__(self):\n",
    "        import os\n",
    "        #存取算法中间结果\n",
    "        self.model_file = './data/chapter3/hmm_model.pkl'\n",
    "        #状态值集合\n",
    "        self.state_list = ['B','M','E','S']\n",
    "        #参数加载，判断是否需要重新加载model_file\n",
    "        self.load_para = False\n",
    "    \n",
    "    #用于加载已计算的中间结果，当需要重新训练时，需要初始化清空结果\n",
    "    def try_load_model(self,trained):\n",
    "        if trained:\n",
    "            import pickle\n",
    "            with open(self.model_file,'rb') as f:\n",
    "                self.A_dic = pickle.load(f)\n",
    "                self.B_dic = pickle.load(f)\n",
    "                self.Pi_dic = pickle.load(f)\n",
    "                self.load_para = True\n",
    "        else:\n",
    "            #状态转移概率（状态->状态的条件概率）\n",
    "            self.A_dic = {}\n",
    "            #发射概率（状态->词语的条件概率）\n",
    "            self.B_dic = {}\n",
    "            #状态的初始概率\n",
    "            self.Pi_dic = {}\n",
    "            self.load_para = False\n",
    "    \n",
    "    #计算转移概率、发射概率以及初始概率\n",
    "    def train(self,path):\n",
    "        #重置三个概率矩阵\n",
    "        self.try_load_model(False)\n",
    "        #统计状态出现次数，求p(o)\n",
    "        Count_dic = {}\n",
    "        #初始化参数\n",
    "        def init_parameters():\n",
    "            for state in self.state_list:\n",
    "                self.A_dic[state] = {s:0.0 for s in self.state_list}\n",
    "                self.B_dic[state] = {}\n",
    "                self.Pi_dic[state] = 0.0\n",
    "                Count_dic[state] = 0\n",
    "        #为每个字标记状态（构词位置）\n",
    "        def makeLabel(text):\n",
    "            out_text = []\n",
    "            if len(text) == 1:\n",
    "                out_text.append('S')\n",
    "            else:\n",
    "                out_text += ['B'] + ['M'] * (len(text)-2) + ['E']\n",
    "            return out_text\n",
    "        init_parameters()\n",
    "        line_num = -1\n",
    "        #\n",
    "        words = set()\n",
    "        with open(path,encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line_num += 1\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                word_list = [i for i in line if i != ' ']\n",
    "                words |= set(word_list) #更新字的集合\n",
    "                linelist = line.split()\n",
    "                line_state = []\n",
    "                for w in linelist:\n",
    "                    line_state.extend(makeLabel(w))\n",
    "                assert len(word_list) == len(line_state), len(word_list)/len(line_state)\n",
    "                for k,v in enumerate(line_state):\n",
    "                    Count_dic[v] += 1\n",
    "                    if k == 0:\n",
    "                        self.Pi_dic[v] += 1    #每个句子第一个字的状态，用于计算初始状态概率\n",
    "                    else:\n",
    "                        self.A_dic[line_state[k-1]][v] += 1 #计算转移概率\n",
    "                        self.B_dic[line_state[k]][word_list[k]] = self.B_dic[line_state[k]].get(word_list[k],0) + 1.0 #计算发射概率\n",
    "\n",
    "            self.Pi_dic = {k : v * 1.0 / line_num for k,v in self.Pi_dic.items() }\n",
    "            self.A_dic = {k : {k1 : v1  / Count_dic[k] for k1,v1 in v.items()} for k,v in self.A_dic.items()}\n",
    "            self.B_dic = {k : {k1 : (v1 + 1)  / Count_dic[k] for k1,v1 in v.items()} for k,v in self.B_dic.items()}\n",
    "            \n",
    "            import pickle\n",
    "            with open(self.model_file, 'wb') as f:\n",
    "                pickle.dump(self.A_dic, f)\n",
    "                pickle.dump(self.B_dic, f)\n",
    "                pickle.dump(self.Pi_dic, f)\n",
    "            return self\n",
    "    \n",
    "    #Viterbi算法实现\n",
    "    def viterbi(self,text,states,start_p,trans_p,emit_p):\n",
    "        V = [{}]        #\n",
    "        path = {}       #\n",
    "        #句首第一个字的状态概率分布\n",
    "        for y in states:\n",
    "            V[0][y] = start_p[y]*emit_p[y].get(text[0],0)\n",
    "            path[y] = [y]\n",
    "        #第二个字开始的状态概率分布    \n",
    "        for t in range(1,len(text)):\n",
    "            V.append({})\n",
    "            newpath = {}\n",
    "            #检验训练的发射概率矩阵中是否有该字\n",
    "            neverSeen = text[t] not in emit_p['S'].keys() and text[t] not in emit_p['B'].keys() and text[t] not in emit_p['M'].keys() and text[t] not in emit_p['E'].keys()\n",
    "            for y in states:\n",
    "                emitP = emit_p[y].get(text[t],0) if not neverSeen else 1.0\n",
    "                (prob,state) = max([(V[t-1][y0] * trans_p[y0].get(y,0) * emitP , y0) for y0 in states if V[t-1][y0] > 0])\n",
    "                V[t][y] = prob\n",
    "                newpath[y] = path[state] + [y]\n",
    "            path = newpath\n",
    "        if emit_p['M'].get(text[-1],0) > emit_p['S'].get(text[-1],0):\n",
    "            (prob,state) = max([(V[len(text)-1],y) for y in ('E','M')])\n",
    "        else:\n",
    "            (prob,state) = max([(V[len(text)-1],y) for y in states])\n",
    "        \n",
    "        return (prob,path[state])\n",
    "    \n",
    "    #切词函数\n",
    "    def cut(self,text):\n",
    "        import os\n",
    "        if not self.load_para:\n",
    "            self.try_load_model(os.path.exists(self.model_file))\n",
    "        prob,pos_list = self.viterbi(text,self.state_list,self.Pi_dic,self.A_dic,self.B_dic)\n",
    "        begin,next = 0,0\n",
    "        for i,char in enumerate(text):\n",
    "            pos = pos_list[i]\n",
    "            if pos == 'B':\n",
    "                begin = i\n",
    "            elif pos == 'E':\n",
    "                yield text[begin:i+1]\n",
    "                next = i+1\n",
    "            elif pos == 'S':\n",
    "                yield char\n",
    "                next = i+1\n",
    "        if next < len(text):\n",
    "            yield text[next:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM()\n",
    "hmm.train('./data/chapter3/trainCorpus.txt_utf8')\n",
    "text = '测试一个句子是否能够被正确分词'\n",
    "#(prob,state) = hmm.viterbi(text,hmm.state_list,hmm.Pi_dic,hmm.A_dic,hmm.B_dic)\n",
    "res = hmm.cut(text)\n",
    "#print(state)\n",
    "print(str(list(res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他统计分词算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>条件随机场（CRF）</b>也是一种基于马尔可夫思想的统计模型。   \n",
    "在HMM中，有一个经典假设，就是每个状态只与它前面的状态有关。这样的假设显然是有偏差的，于是学者们提出了条件随机场算法，使得每个状态不止与他前面的状态有关，还与他后面的状态有关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最常用的方法是：先基于词典的方式进行分词，然后再用统计分词方法进行辅助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文分词工具——jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba分词结合了基于规则和基于统计这两类方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba的三种分词模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>精确模式</b>：试图将句子最精确地切开，适合文本分析\n",
    "- <b>全模式</b>：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义\n",
    "- <b>搜索引擎模式</b>：在精确模式的基础上，对长词再次切分，提高召回率，适用于搜索引擎分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "sent = '中文分词是文本处理不可或缺的一步！'\n",
    "seg_list = jieba.cut(sent,cut_all=True)\n",
    "print('全模式：','/'.join(seg_list))\n",
    "seg_list = jieba.cut(sent,cut_all=False)\n",
    "print('精确模式：','/'.join(seg_list))\n",
    "seg_list = jieba.cut(sent)\n",
    "print('精确模式（默认）：','/'.join(seg_list))\n",
    "seg_list = jieba.cut_for_search(sent)\n",
    "print('搜索引擎模式：','/'.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实战之高频词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高频词提取其实就是NLP中的TF(Term Frequency)策略，主要有以下干扰项：\n",
    "- 标点符号\n",
    "- 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义数据读取函数\n",
    "def get_content(path):\n",
    "    with open(path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "        content = ''\n",
    "        for l in f:\n",
    "            l = l.strip()\n",
    "            content += l\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义高频词统计函数\n",
    "def get_TF(words,topK=10):\n",
    "    tf_dic = {}\n",
    "    for w in words:\n",
    "        tf_dic[w] = tf_dic.get(w,0) + 1\n",
    "    return sorted(tf_dic.items(), key=lambda x:x[1],reverse=True)[:topK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义停用词函数\n",
    "def stop_words(path):\n",
    "    with open(path) as f:\n",
    "        return [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import glob\n",
    "    import random\n",
    "    import jieba\n",
    "\n",
    "    files = glob.glob('./data/chapter3/news/C000013/*.txt')\n",
    "    corpus = [get_content(x) for x in files]\n",
    "    \n",
    "    sample_inx = random.randint(0,len(corpus))\n",
    "    split_words = [x for x in jieba.cut(corpus[sample_inx]) if x not in stop_words('./data/chapter3/stop_words.utf8')]\n",
    "    print('样本之一：'+corpus[sample_inx])\n",
    "    print('样本分词效果：'+'/'.join(split_words))\n",
    "    print('样本的topK(10)词：'+str(get_TF(split_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词性标注与命名实体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba分词中的词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pair('中文', 'nz'), pair('分词', 'n'), pair('是', 'v'), pair('文本处理', 'n'), pair('不可或缺', 'l'), pair('的', 'uj'), pair('一步', 'm'), pair('！', 'x')]\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as psg\n",
    "sent = '中文分词是文本处理不可或缺的一步！'\n",
    "seg_list = psg.cut(sent)\n",
    "#print(str(list(seg_list)))\n",
    "print(' '.join(['{0}/{1}'.format(w,t) for w,t in seg_list]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
